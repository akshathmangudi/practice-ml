{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Spam Email Classification\n\nIn this notebook, we will explore the Spam Email dataset, where we will train a machine learning model to classify whether a set of mails are spam or not spam. We will be using the Random Forest Classifier for this algorithm, which is handy for classification problems like this. \n\nRandom Forest Algorithms or RFs for short, tend to find decision boundaries between features that allow us to classify accurately and achieve a small error rate in result.","metadata":{}},{"cell_type":"markdown","source":"### Step 1: Importing the necessary libraries\n\nSince this dataset does not involve any sort of numerical data, there's only three steps to this notebook. \n\n1. CSV I/O\n2. Tokenizing or Vectorizing\n3. Training the model","metadata":{}},{"cell_type":"code","source":"import pandas as pd # For CSV I/O\nimport numpy as np # For pandas dataframe manipulation\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-26T19:02:37.052312Z","iopub.execute_input":"2023-09-26T19:02:37.053012Z","iopub.status.idle":"2023-09-26T19:02:37.495951Z","shell.execute_reply.started":"2023-09-26T19:02:37.052968Z","shell.execute_reply":"2023-09-26T19:02:37.494650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/spam-email-dataset/emails.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T19:02:37.503901Z","iopub.execute_input":"2023-09-26T19:02:37.504717Z","iopub.status.idle":"2023-09-26T19:02:37.639035Z","shell.execute_reply.started":"2023-09-26T19:02:37.504667Z","shell.execute_reply":"2023-09-26T19:02:37.637783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Resets the old index of the dataframe and samples the entire dataframe.\ndata = data.sample(frac=1).reset_index(drop=True)\ndisplay(data)\nclass_names=['Not Spam', 'Spam']","metadata":{"execution":{"iopub.status.busy":"2023-09-26T19:02:37.640704Z","iopub.execute_input":"2023-09-26T19:02:37.641386Z","iopub.status.idle":"2023-09-26T19:02:37.662597Z","shell.execute_reply.started":"2023-09-26T19:02:37.641344Z","shell.execute_reply":"2023-09-26T19:02:37.660427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting into text and spam, or formally our dependent and independet variables.\nX_data = data['text']\ny_data = data['spam']","metadata":{"execution":{"iopub.status.busy":"2023-09-26T19:02:37.668568Z","iopub.execute_input":"2023-09-26T19:02:37.668982Z","iopub.status.idle":"2023-09-26T19:02:37.674451Z","shell.execute_reply.started":"2023-09-26T19:02:37.668947Z","shell.execute_reply":"2023-09-26T19:02:37.673140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2: Tokenizing / Vectorizing\n\nWhat is the difference? \n\nYou might've noticed that I used sklearn.feature_extraction.text.CountVectorizer(), however I have also used keras.text_preprocessing.Tokenizer() in the past with other datasets that were more NLP-centeric. What to use when?\n\nIt comes down to the use case. As I am doing simple text classification, scikit-learn's version is preferred. However, under NLP use cases or if a deep learning model is in use, then pairing the model with frameworks like keras/tensorflow works better. ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import KFold, train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-09-26T19:02:37.676091Z","iopub.execute_input":"2023-09-26T19:02:37.677371Z","iopub.status.idle":"2023-09-26T19:02:38.391569Z","shell.execute_reply.started":"2023-09-26T19:02:37.677314Z","shell.execute_reply":"2023-09-26T19:02:38.390057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are printing the size of the array of the vectorized data and the actual data here. ","metadata":{}},{"cell_type":"code","source":"count_vect = CountVectorizer()\nX_vect = count_vect.fit_transform(X_data)\nprint(X_vect.shape, X_data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T19:02:38.393440Z","iopub.execute_input":"2023-09-26T19:02:38.393969Z","iopub.status.idle":"2023-09-26T19:02:39.749711Z","shell.execute_reply.started":"2023-09-26T19:02:38.393924Z","shell.execute_reply":"2023-09-26T19:02:39.748363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 3: Model Training\nWe will be using train_test_split to split the data into training and testing data and use our RFC algorithm to fit X_train and y_train. \n\nArguments: \n1. random_state:- Controls the randomness when building trees as well as the sampling of features when looking for the best split at each node.\n\n2. max_features:- the number of features to look for when looking for the best decision boundary (right from sklearn's documentation). There's two possible arguments for this. \"sqrt\" or \"log2\", as well as a \"None\" argument. \n\nI tested \"log2\" first, and I got a 91% classification on Not-Spam, while \"sqrt\" performed significantly better, with 95% precision.","metadata":{}},{"cell_type":"code","source":"results = []\n\nX_train, X_test, y_train, y_test = train_test_split(X_vect, y_data, test_size=0.3, random_state=64)\n\nmodel = RandomForestClassifier(random_state=64, max_features='sqrt')\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T19:02:39.751364Z","iopub.execute_input":"2023-09-26T19:02:39.751822Z","iopub.status.idle":"2023-09-26T19:02:44.439711Z","shell.execute_reply.started":"2023-09-26T19:02:39.751780Z","shell.execute_reply":"2023-09-26T19:02:44.438583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We are rounding our values and getting ready for the classification report.\ny_pred = model.predict(X_test)\nresults.extend(y_pred)\ny_pred2 = np.round(y_pred, 0)\n\ny_pred = np.round(results, 0)\ny_true = y_data","metadata":{"execution":{"iopub.status.busy":"2023-09-26T19:02:44.440993Z","iopub.execute_input":"2023-09-26T19:02:44.441504Z","iopub.status.idle":"2023-09-26T19:02:44.581439Z","shell.execute_reply.started":"2023-09-26T19:02:44.441471Z","shell.execute_reply":"2023-09-26T19:02:44.580214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred2, target_names=class_names, digits=3))","metadata":{"execution":{"iopub.status.busy":"2023-09-26T19:02:44.583191Z","iopub.execute_input":"2023-09-26T19:02:44.583843Z","iopub.status.idle":"2023-09-26T19:02:44.600060Z","shell.execute_reply.started":"2023-09-26T19:02:44.583810Z","shell.execute_reply":"2023-09-26T19:02:44.598608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the end of the notebook. I hope you got a glimpse of how machine learning algorithms are implemented on a dataset like this. For any queries, email: akshathmangudi@gmail.com\n\nNotebook by Akshath Mangudi","metadata":{}}]}