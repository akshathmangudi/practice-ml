{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-02T06:27:51.812783Z","iopub.execute_input":"2023-03-02T06:27:51.813932Z","iopub.status.idle":"2023-03-02T06:27:51.830423Z","shell.execute_reply.started":"2023-03-02T06:27:51.813884Z","shell.execute_reply":"2023-03-02T06:27:51.828809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this notebook, you will find the code to predicting breast cancer in a datatset using the Support Vector Machines classifier. The algorithm, finds a hyperplane between two classifications in a higher dimensional space by using a kernel function. We will also look at the implementation of Logistic Regression, as both the algorithms are designed for classifcation and compare the accuracy scores. ","metadata":{}},{"cell_type":"markdown","source":"**Step 1: Import the necesary modules.**","metadata":{}},{"cell_type":"code","source":"import numpy as np # For efficiently carrying out array computations \nimport pandas as pd # For CSV I/O \nimport matplotlib.pyplot as plt # For data visualization. \nimport seaborn as sns # For data visualization as well.\n\n# Importing the necessary algorithms and error functions. \nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler # For feature scaling\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n# For splitting the datasets and finding the best model using GridSearch\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n# For finding the error of the algorithm trained. \nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n# The algorithm itself. \n","metadata":{"execution":{"iopub.status.busy":"2023-03-02T06:27:51.832534Z","iopub.execute_input":"2023-03-02T06:27:51.832959Z","iopub.status.idle":"2023-03-02T06:27:51.840850Z","shell.execute_reply.started":"2023-03-02T06:27:51.832919Z","shell.execute_reply":"2023-03-02T06:27:51.839501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 2: Loading the data and getting a feel for it.**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/breast-cancer-dataset/breast-cancer.csv') # Loading the data\ndf.head()\n# df.describe() ","metadata":{"execution":{"iopub.status.busy":"2023-03-02T06:27:51.888801Z","iopub.execute_input":"2023-03-02T06:27:51.889777Z","iopub.status.idle":"2023-03-02T06:27:51.926928Z","shell.execute_reply.started":"2023-03-02T06:27:51.889725Z","shell.execute_reply":"2023-03-02T06:27:51.925609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 3: Analyze the dataset.** \n* This involves various data visualizations to find the hidden relationships between each variable. ","metadata":{}},{"cell_type":"code","source":"df['diagnosis'] = [1 if i == \"M\" else 0 for i in df['diagnosis']]\ndf.drop('id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T06:27:51.930065Z","iopub.execute_input":"2023-03-02T06:27:51.930471Z","iopub.status.idle":"2023-03-02T06:27:51.971024Z","shell.execute_reply.started":"2023-03-02T06:27:51.930433Z","shell.execute_reply":"2023-03-02T06:27:51.969664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df.corr()\nfig = plt.figure(figsize = (20, 16))\nsns.heatmap(corr, annot=True, \n           cmap='magma', \n           fmt=\".1f\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-02T06:27:51.972927Z","iopub.execute_input":"2023-03-02T06:27:51.973324Z","iopub.status.idle":"2023-03-02T06:27:56.524990Z","shell.execute_reply.started":"2023-03-02T06:27:51.973284Z","shell.execute_reply":"2023-03-02T06:27:56.524104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.kdeplot(df['radius_mean'], label=\"Radius\")\nsns.kdeplot(df['perimeter_mean'], label = \"Perimeter\")\nsns.kdeplot(df['texture_mean'], label = \"Texture\")\nax.legend()","metadata":{"execution":{"iopub.status.busy":"2023-03-02T06:27:56.526292Z","iopub.execute_input":"2023-03-02T06:27:56.527061Z","iopub.status.idle":"2023-03-02T06:27:56.837256Z","shell.execute_reply.started":"2023-03-02T06:27:56.527020Z","shell.execute_reply":"2023-03-02T06:27:56.836353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 4: Preprocess the data**\n* This involves feature scaling so that the algorithm works better for the given dataset. ","metadata":{}},{"cell_type":"code","source":"corr_val = abs(corr[\"diagnosis\"])\n#Select only the highly correlated values. \nrelevant = corr_val[corr_val > 0.4]\nrelevant = list(corr_val.index)\nrelevant.remove(\"id\")\nrelevant.remove(\"diagnosis\")\n\nX = df[relevant]\nY = df[\"diagnosis\"]\n\n# print(X)\n# print(Y)\n\n# We got confirmation that by printing these lists, we have the values we need. ","metadata":{"execution":{"iopub.status.busy":"2023-03-02T06:27:56.840014Z","iopub.execute_input":"2023-03-02T06:27:56.840690Z","iopub.status.idle":"2023-03-02T06:27:56.849195Z","shell.execute_reply.started":"2023-03-02T06:27:56.840649Z","shell.execute_reply":"2023-03-02T06:27:56.847939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, we will perform feature scaling for the dataset \"X\" using StandardScaler and convert the list to a dataframe\nX = StandardScaler().fit_transform(X) # mean = 0, standard deviation = 1\nX = pd.DataFrame(X)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-02T06:27:56.850839Z","iopub.execute_input":"2023-03-02T06:27:56.851184Z","iopub.status.idle":"2023-03-02T06:27:56.889898Z","shell.execute_reply.started":"2023-03-02T06:27:56.851151Z","shell.execute_reply":"2023-03-02T06:27:56.888823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 5: Splitting the training and testing data, as well as finding the best model parameters for SVM.**\n* This is the second-to-last step before completing the program, whewre the last step will be to find the model with the least amount of error and print that onto the console. ","metadata":{}},{"cell_type":"code","source":"# Splitting X and Y into training and testing datasets\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T06:27:56.891769Z","iopub.execute_input":"2023-03-02T06:27:56.892146Z","iopub.status.idle":"2023-03-02T06:27:56.900385Z","shell.execute_reply.started":"2023-03-02T06:27:56.892111Z","shell.execute_reply":"2023-03-02T06:27:56.899094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To find the best model, we use a variety of arguments. \ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # For splitting the datasets in the same proportion\nsvc = SVC()\nsvc_args = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': [1, 0.1, 0.01, 0.001],\n    'kernel': ['rbf', 'poly', 'sigmoid']\n}\n\nbest_model = GridSearchCV(estimator = svc,\n                          param_grid = svc_args,\n                          cv = cv, \n                          verbose = 1, \n                          scoring = 'roc_auc')\n\nresult_svc = best_model.fit(x_train, y_train)\nresult_svc.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-03-02T06:27:56.901776Z","iopub.execute_input":"2023-03-02T06:27:56.902103Z","iopub.status.idle":"2023-03-02T06:27:59.558070Z","shell.execute_reply.started":"2023-03-02T06:27:56.902072Z","shell.execute_reply":"2023-03-02T06:27:59.556825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result: \n* C: 100\n* gamma: 0.001\n* kernel: rbf (radial basis function)\n\nare the three best fitted parameters for the Support Vectors Classifier. It was much easier to use GridSearchCV than manually altering each C, gamma and kernel value in \norder to get the right output. ","metadata":{}},{"cell_type":"code","source":"# Using the best parameters gotten from GridSearch, we are training the SVC algorithm on those params. \nsvc = svc.set_params(**result_svc.best_params_)\nsvc.fit(x_train, y_train)\n\n#Getting the parameters and finding the predictions using the testing dataset.\nprediction = svc.predict(x_test) \nprint(classification_report(y_test, prediction))\nprint(confusion_matrix(y_test, prediction))\nprint(f\"ROC-AUC Score: {roc_auc_score(y_test, prediction)}\")\nprint(f\"Accuracy Score: {accuracy_score(y_test, prediction)}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-02T06:27:59.559598Z","iopub.execute_input":"2023-03-02T06:27:59.560020Z","iopub.status.idle":"2023-03-02T06:27:59.584653Z","shell.execute_reply.started":"2023-03-02T06:27:59.559981Z","shell.execute_reply":"2023-03-02T06:27:59.583413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, we have ended up with a 97% accuracy score, which is quite good. The algorithm has done well. ","metadata":{}},{"cell_type":"markdown","source":"**COMPARISON: Logistic Regression vs. Support Vector Machine**\n* This is an extra: This isn't required. Both algorithms are used for classifiacation problems, and to me at least, it's interesting to see which algorithm does the job better. ","metadata":{}},{"cell_type":"code","source":"log_regr = LogisticRegression() \n\nlog_args = {\n    'penalty':['l1', 'l2'],\n    'C': [0.001, 0.01, 0.1, 0.9, 1, 2, 10, 100],\n    'solver': ['newton-cg', 'lbfgs''liblinear', 'sag', 'saga']\n}\n\ngrid_lgr = GridSearchCV(estimator = log_regr, \n                        param_grid = log_args, \n                        cv = cv, \n                        verbose = 1, \n                        scoring = 'roc_auc')\nresult_lgr = grid_lgr.fit(x_train, y_train)\nresult_lgr.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-03-02T06:27:59.585973Z","iopub.execute_input":"2023-03-02T06:27:59.586312Z","iopub.status.idle":"2023-03-02T06:28:04.637022Z","shell.execute_reply.started":"2023-03-02T06:27:59.586278Z","shell.execute_reply":"2023-03-02T06:28:04.635117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_regr = log_regr.set_params(**result_lgr.best_params_)\nlog_regr.fit(x_train, y_train)\nprediction = log_regr.predict(x_test)\n\nprint(classification_report(y_test, prediction))\nprint(confusion_matrix(y_test, prediction))\nprint(f\"ROC-AUC Score: {roc_auc_score(y_test, prediction)}\")\nprint(f\"Accuracy Score: {accuracy_score(y_test, prediction)}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-02T06:28:04.639904Z","iopub.execute_input":"2023-03-02T06:28:04.641272Z","iopub.status.idle":"2023-03-02T06:28:04.712105Z","shell.execute_reply.started":"2023-03-02T06:28:04.641191Z","shell.execute_reply":"2023-03-02T06:28:04.710159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Final Results**: \n* Accuracy Score (SVC): 97.902%\n* Accuracy Score (LGR): 97.902%\n\nBoth the classification algorithms have the same accuracy score and very similar confusion matrices, with SVC showing more precision but insignificantly. ","metadata":{}},{"cell_type":"markdown","source":"* Notebook by Akshath Mangudi\n* Inspiration and references drawn from: https://www.kaggle.com/code/gevorgakopyan/98-7-breast-cancer-dataset-svm-knn-randomforest\n* **End of Notebook**","metadata":{}}]}